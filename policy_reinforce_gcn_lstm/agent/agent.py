import sys
import os
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from itertools import combinations
from model.model_gcn_lstm import PolicyGCNLSTM as Policy
import torch
from torch import optim
from utils import get_device
from torch_geometric.data import Data
import numpy as np

class Agent:
    def __init__(self, lr=1e-4, gamma=0.99, hidden_size=4):
        self.gamma = gamma  # å‰²å¼•ç‡
        self.lr = lr  # å­¦ç¿’ç‡
        self.memory = []
        self.device = get_device()
        self.pi = Policy(hidden_size=hidden_size).to(self.device)  # ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.optimizer = optim.Adam(self.pi.parameters(), lr=self.lr)

    def get_action(self, data, state):
        
        # print('state:')
        # print(state)

        # ã™ã§ã« tensor ãªã®ã§ãã®ã¾ã¾ä½¿ã£ã¦ OKï¼
        state = state.unsqueeze(0).to(self.device)  # shape: (1, 5, 3)

        # è¨ªå•æ¸ˆã¿éƒ½å¸‚ã®ãƒ•ãƒ©ã‚°ã‚’å–ã‚Šå‡ºã™ï¼ˆ3åˆ—ç›®ï¼‰
        visited_cities = state[:, :, 3].squeeze()   # shape: (5,)
        # visited: tensor([0., 1., 0., 0., 0.], dtype=torch.float32)
        visited_indices = (visited_cities == 1).nonzero(as_tuple=True)[0]  # â† LongTensorã«ãªã‚‹ï¼

        # ãƒ­ã‚°ï¼ˆç¢ºèªç”¨ï¼‰
        # print(f'visited_cities: {visited_cities}')
        # print(f'visited_indices: {visited_indices}')
        action, probs = self.pi(data, visited_indices)  # éƒ½å¸‚ã®è¨ªå•ç¢ºç‡ã‚’å–å¾—
        # print('action_logits:')
        # print(action_logits)
 
        return action, probs
    
    def add(self, reward, action_probs):
        self.memory.append((reward, action_probs))

    def update(self):
        G, loss = 0, 0
        EPS = 1e-8  # æ¥µå°å€¤ã‚’å®šç¾©

        for reward, action_probs in reversed(self.memory):
            # print(f"action_probs: {action_probs}")
            G = reward + self.gamma * G
            loss += -torch.log(action_probs + EPS) * G  # EPSãŒãªã„ã¨log(0)ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
        
        loss = loss.sum()
        
        self.optimizer.zero_grad()
        # **å­¦ç¿’å‰ã®é‡ã¿ã‚’è¡¨ç¤º**
        # print("ğŸ” LSTM weights BEFORE update:")
        # for name, param in self.pi.lstm.named_parameters():
        #     print(f"{name}: {param.data}")
            
        loss.backward()
        self.optimizer.step()

        # **å­¦ç¿’å¾Œã®é‡ã¿ã‚’è¡¨ç¤º**
        # print("ğŸ” LSTM weights AFTER update:")
        # for name, param in self.pi.lstm.named_parameters():
        #     print(f"{name}: {param.data}")
            
        self.memory = []
    
    # modelã‚’ä¿å­˜ã™ã‚‹é–¢æ•°
    def save_model(self, path):
        torch.save(self.pi.state_dict(), path)
    
    # modelã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°
    def load_model(self, path):
        self.pi.load_state_dict(torch.load(path, weights_only=True))
    

def main():
    agent = Agent()

    device = get_device()

    state = torch.tensor([
        [95.02370691, 54.86014805, 1],
        [79.64110929, 9.31860416, 1],
        [37.19301454, 14.77507159, 0],
        [62.41494659, 80.49261453, 0],
        [28.36552228, 90.45835087, 0]
    ], dtype=torch.float32)

    # éƒ½å¸‚æ•°
    num_nodes = state.shape[0]

    # ã™ã¹ã¦ã®éƒ½å¸‚ãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆi â‰  jï¼‰
    edges = list(combinations(range(num_nodes), 2))

    # edge_index ã®ä½œæˆ
    edge_index = torch.tensor(edges, dtype=torch.long).T  # è»¢ç½®ã—ã¦ (2, N) ã®å½¢ã«ã™ã‚‹

    # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’è¨ˆç®—ã—ã¦ã‚¨ãƒƒã‚¸ã®é‡ã¿ã¨ã™ã‚‹
    edge_weight = torch.norm(state[edge_index[0]] - state[edge_index[1]], dim=1)

    # Min-Maxã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¿½åŠ  (0~1ã«æ­£è¦åŒ–)
    edge_weight = (edge_weight - edge_weight.min()) / (edge_weight.max() - edge_weight.min())

    # `torch_geometric.data.Data` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
    data = Data(x=state.to(device), edge_index=edge_index.to(device), edge_weight=edge_weight.to(device))
    print('data:')
    print(data, state)
    
    nex_node, probs = agent.get_action(data, state)
    print('next_node:', nex_node)
    print('probs:', probs)

if __name__ == '__main__':
    main()